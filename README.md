# MiniChatLLM

ä¸€ä¸ªåŸºäºŽ **Qt + llama.cpp** çš„æœ¬åœ°å¤§è¯­è¨€æ¨¡åž‹ï¼ˆLLMï¼‰æ¡Œé¢èŠå¤© Demoã€‚  
æ”¯æŒ **Qwen-1.5** å’Œ **Yi-1.5**ï¼ˆGGUF æ ¼å¼ï¼‰æ¨¡åž‹ï¼Œæä¾›åŸºç¡€æŽ¨ç†å’Œå¢žå¼ºé‡‡æ ·æ¨¡å¼ã€‚

---

## âœ¨ åŠŸèƒ½ç‰¹ç‚¹

- **æœ¬åœ°æŽ¨ç†** â€” åŸºäºŽ [llama.cpp](https://github.com/ggerganov/llama.cpp)ï¼Œæ— éœ€è”ç½‘å³å¯è¿è¡Œã€‚
- **Qt å›¾å½¢ç•Œé¢** â€” è¾“å…¥é—®é¢˜å¹¶æ˜¾ç¤ºå›žå¤ã€‚
- **æ¨¡åž‹åŠ è½½è¿›åº¦æç¤º** â€” å¸¦æœ‰å·²ç”¨ç§’æ•°çš„åŠ è½½å¯¹è¯æ¡†ã€‚
- **ä¸¤ç§æŽ¨ç†åŽç«¯**ï¼š
  - `LLMRunner` â€” ç®€å•è´ªå¿ƒè§£ç ã€‚
  - `LLMRunnerYi` â€” Top-p + Temperature é‡‡æ ·ï¼Œé€‚é… Yi-1.5-6B-Chatã€‚
- **æµå¼è¾“å‡º** â€” å®žæ—¶è¾“å‡ºç”Ÿæˆç‰‡æ®µã€‚
- **å¤šçº¿ç¨‹æ‰§è¡Œ** â€” æŽ¨ç†åœ¨åŽå°çº¿ç¨‹è¿è¡Œï¼Œä¿è¯ç•Œé¢æµç•…ã€‚
- **å¯¹è¯åŽ†å²è®°å½•** â€” è‡ªåŠ¨ä¿ç•™ä¸Šä¸‹æ–‡ã€‚

---

## ðŸ“‚ é¡¹ç›®ç»“æž„

```
MiniChatLLM/
â”œâ”€â”€ ChatWindow.h / ChatWindow.cpp     # ä¸»èŠå¤©ç•Œé¢
â”œâ”€â”€ LLMRunner.h / LLMRunner.cpp       # Qwen / ç®€å•è§£ç  LLM åŽç«¯
â”œâ”€â”€ LLMRunnerYi.h / LLMRunnerYi.cpp   # Yi-1.5 ä¸“ç”¨é‡‡æ ·åŽç«¯
â”œâ”€â”€ LoadingTipWidget.h / .cpp         # æ¨¡åž‹åŠ è½½æç¤ºæŽ§ä»¶
â”œâ”€â”€ stdafx.h / stdafx.cpp             # å…¬å…±å¤´æ–‡ä»¶ä¸Žå·¥å…·å‡½æ•°
â””â”€â”€ models/                           # å­˜æ”¾ .gguf æ¨¡åž‹æ–‡ä»¶
```

---

## ðŸ›  ä¾èµ–çŽ¯å¢ƒ

- **C++17** æˆ–æ›´é«˜ç‰ˆæœ¬
- [Qt 5.15+](https://www.qt.io/download-qt-installer)
- [llama.cpp (2024.06+)](https://github.com/ggerganov/llama.cpp) ç¼–è¯‘åŽçš„åº“æ–‡ä»¶  
- ä¸€ä¸ª **GGUF** æ¨¡åž‹æ–‡ä»¶ï¼ˆå¦‚ Qwen1.5-0.5B-Chat-Q4_0.ggufã€Yi-1.5-6B-Chat-Q4_K_M.ggufï¼‰

---

## âš™ï¸ ç¼–è¯‘æ­¥éª¤

1. **å…‹éš†é¡¹ç›®**  
   ```bash
   git clone https://github.com/yourname/MiniChatLLM.git
   cd MiniChatLLM
   ```

2. **ç¼–è¯‘ llama.cpp** å¹¶æ‹·è´å¤´æ–‡ä»¶ä¸Žåº“æ–‡ä»¶åˆ°é¡¹ç›®ä¸­ï¼š  
   ```bash
   cmake -B build -DLLAMA_BUILD=OFF
   cmake --build build --config Release
   ```
   æ”¾ç½®ï¼š
   - `llama.h` åˆ° `llama/include`
   - `llama.lib` / `llama.dll`ï¼ˆWindowsï¼‰æˆ– `.a` / `.so`ï¼ˆLinuxï¼‰åˆ° `llama/lib`

3. **ä½¿ç”¨ Qt Creator / Visual Studio æ‰“å¼€å·¥ç¨‹**  
   - ç¡®ä¿å·²é“¾æŽ¥ `llama` åº“è·¯å¾„ã€‚
   - è®¾ç½®ä¸º **x64** æž„å»ºæ¨¡å¼ã€‚
   - ä¿®æ”¹ `ChatWindow.cpp` ä¸­æ¨¡åž‹è·¯å¾„ä¸ºä½ æœ¬åœ° `.gguf` æ–‡ä»¶è·¯å¾„ã€‚

4. **ç¼–è¯‘å¹¶è¿è¡Œ**  
   é¦–æ¬¡è¿è¡Œä¼šåŠ è½½æ¨¡åž‹ï¼ˆè€—æ—¶å–å†³äºŽæ¨¡åž‹å¤§å°å’Œç¡¬ä»¶æ€§èƒ½ï¼‰ã€‚

---

## ðŸš€ ä½¿ç”¨æ–¹æ³•

1. å¯åŠ¨ç¨‹åºã€‚
2. ç­‰å¾… **â€œåŠ è½½æ¨¡åž‹ä¸­...â€** å¯¹è¯æ¡†å®Œæˆã€‚
3. åœ¨è¾“å…¥æ¡†è¾“å…¥é—®é¢˜ã€‚
4. æŒ‰ **Enter** æˆ–ç‚¹å‡» **å‘é€**ã€‚
5. AI å›žå¤ä¼šæ˜¾ç¤ºåœ¨èŠå¤©çª—å£ã€‚

---

## ðŸ”„ åˆ‡æ¢æ¨¡åž‹

- ä½¿ç”¨ Qwenï¼ˆç®€å•è§£ç ï¼‰ï¼š  
  åœ¨ `stdafx.h` ä¸­è®¾ç½® `USE_SIMPLE 1`ï¼Œå¹¶ä½¿ç”¨ `LLMRunner`ã€‚
- ä½¿ç”¨ Yi-1.5ï¼ˆå¸¦é‡‡æ ·ï¼‰ï¼š  
  è®¾ç½® `USE_SIMPLE 0`ï¼Œå¹¶ä½¿ç”¨ `LLMRunnerYi`ã€‚

---

## ðŸ“Œ æ³¨æ„äº‹é¡¹

- æ€§èƒ½ä¾èµ–äºŽç¡¬ä»¶å’Œæ¨¡åž‹å¤§å°ã€‚
- å¤§æ¨¡åž‹å¯èƒ½éœ€è¦ >8GB æ˜¾å­˜æˆ–åœ¨ CPU ä¸Šè¿è¡Œè¾ƒæ…¢ã€‚
- æŽ¨èä½¿ç”¨é‡åŒ–æ¨¡åž‹ï¼ˆ`Q4_0`ã€`Q5_K_M` ç­‰ï¼‰ä»¥æå‡é€Ÿåº¦ã€‚

---

## ðŸ“œ è®¸å¯è¯

æœ¬é¡¹ç›®ä½¿ç”¨ MIT License è®¸å¯ã€‚  
llama.cpp ä½¿ç”¨å…¶åŽŸå§‹è®¸å¯è¯ã€‚

---

## ðŸ™‹ è‡´è°¢

- [ggerganov/llama.cpp](https://github.com/ggerganov/llama.cpp)
- [Qt Project](https://www.qt.io/)


# MiniChatLLM 

A minimal **Qt + llama.cpp** desktop demo for running local Large Language Models (LLMs) in an interactive chat interface.  
Supports **Qwen-1.5** and **Yi-1.5** models (GGUF format) with basic and enhanced sampling.

---

## âœ¨ Features

- **Local inference** using [llama.cpp](https://github.com/ggerganov/llama.cpp) â€” no network required.
- **Qt-based GUI** for sending messages and displaying responses.
- **Model loading progress dialog** with elapsed time counter.
- **Two inference backends**:
  - `LLMRunner` â€” simple greedy decoding.
  - `LLMRunnerYi` â€” top-p + temperature sampling, tuned for Yi-1.5-6B-Chat.
- **Streaming output** â€” partial results emitted in real time.
- **Threaded execution** â€” model runs in a background thread, keeping the UI responsive.
- **Conversation history** â€” context is maintained across turns.

---

## ðŸ“‚ Project Structure

```
MiniChatLLM/
â”œâ”€â”€ ChatWindow.h / ChatWindow.cpp     # Main chat UI
â”œâ”€â”€ LLMRunner.h / LLMRunner.cpp       # Qwen/simple LLM backend
â”œâ”€â”€ LLMRunnerYi.h / LLMRunnerYi.cpp   # Yi-1.5-specific backend with sampling
â”œâ”€â”€ LoadingTipWidget.h / .cpp         # Model loading indicator widget
â”œâ”€â”€ stdafx.h / stdafx.cpp             # Common includes & helpers
â””â”€â”€ models/                           # Place .gguf models here
```

---

## ðŸ›  Dependencies

- **C++17** or newer
- [Qt 5.15+](https://www.qt.io/download-qt-installer)
- [llama.cpp (2024.06+)](https://github.com/ggerganov/llama.cpp) built as a static or dynamic library  
- A **GGUF** model file (e.g., Qwen1.5-0.5B-Chat-Q4_0.gguf, Yi-1.5-6B-Chat-Q4_K_M.gguf)

---

## âš™ï¸ Build Instructions

1. **Clone this repo**  
   ```bash
   git clone https://github.com/yourname/MiniChatLLM.git
   cd MiniChatLLM
   ```

2. **Build llama.cpp** and copy the headers & library into the project:  
   ```bash
   cmake -B build -DLLAMA_BUILD=OFF
   cmake --build build --config Release
   ```
   Place:
   - `llama.h` into `llama/include`
   - `llama.lib` / `llama.dll` (Windows) or `.a` / `.so` (Linux) into `llama/lib`

3. **Open in Qt Creator / Visual Studio**  
   - Ensure the `llama` library path is linked.
   - Set build target to **x64**.
   - Update the model path in `ChatWindow.cpp` to your local `.gguf` file.

4. **Build & run**  
   The first run will load the model (time depends on model size and hardware).

---

## ðŸš€ Usage

1. Launch the application.
2. Wait for the **"Loading model..."** dialog to finish.
3. Type your question in the input box.
4. Press **Enter** or click **Send**.
5. AI responses will appear in the chat view.

---

## ðŸ”„ Switching Models

- To use Qwen (simple decoding):  
  Set `USE_SIMPLE 1` in `stdafx.h` and use `LLMRunner`.
- To use Yi-1.5 with sampling:  
  Set `USE_SIMPLE 0` and use `LLMRunnerYi`.

---

## ðŸ“Œ Notes

- Performance depends heavily on hardware and model size.
- Large models may require >8GB VRAM or run slower on CPU.
- For faster responses, use quantized models (`Q4_0`, `Q5_K_M`, etc.).

---

## ðŸ“œ License

This project is released under the MIT License.  
llama.cpp is under its respective license.

---

## ðŸ™‹ Acknowledgements

- [ggerganov/llama.cpp](https://github.com/ggerganov/llama.cpp)
- [Qt Project](https://www.qt.io/)
